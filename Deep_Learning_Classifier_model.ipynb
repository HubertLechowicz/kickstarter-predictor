{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Classifier model",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuatZo/kickstarter-predictor/blob/master/Deep_Learning_Classifier_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX5bL75o4JVB",
        "colab_type": "text"
      },
      "source": [
        "# Import libraries & dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtqRfOuDl5nn",
        "colab_type": "code",
        "outputId": "29ac58a2-c8ed-4191-e14c-236dfa0494cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import statistics as stats\n",
        "import requests\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.base import clone\n",
        "from sklearn.externals.joblib import dump, load\n",
        "\n",
        "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC, ADASYN, BorderlineSMOTE\n",
        "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
        "\n",
        "df = pd.read_csv('http://puma.swstats.info/files/kickstarter_with_trends.csv', index_col=\"ID\")\n",
        "df.columns"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['name', 'category', 'main_category', 'currency', 'deadline', 'goal',\n",
              "       'launched', 'pledged', 'state', 'backers', 'country', 'usd pledged',\n",
              "       'usd_pledged_real', 'usd_goal_real', 'tokenized_name', 'trend'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd8ixBY6a0LY",
        "colab_type": "text"
      },
      "source": [
        "# Prepare for data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrx5fuwJa0P-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://3l7z4wecia.execute-api.us-east-1.amazonaws.com/default/api-dynamodb/'\n",
        "\n",
        "get_categories = {\n",
        "  \"operation\": \"list\",\n",
        "  \"table\": \"categories\",\n",
        "}\n",
        "\n",
        "categories = requests.post(link, json=get_categories)\n",
        "\n",
        "categories = categories.json()['Items']\n",
        "categories_proper = dict()\n",
        "for item in categories:\n",
        "  categories_proper[item['name']] = item['id'] # map NAME to ID\n",
        "\n",
        "get_main_categories = {\n",
        "  \"operation\": \"list\",\n",
        "  \"table\": \"maincategories\",\n",
        "}\n",
        "\n",
        "main_categories = requests.post(link, json=get_main_categories)\n",
        "\n",
        "main_categories = main_categories.json()['Items']\n",
        "main_categories_proper = dict()\n",
        "for item in main_categories:\n",
        "  main_categories_proper[item['name']] = item['id'] # map NAME to ID\n",
        "\n",
        "get_countries = {\n",
        "  \"operation\": \"list\",\n",
        "  \"table\": \"countries\",\n",
        "}\n",
        "\n",
        "countries = requests.post(link, json=get_countries)\n",
        "\n",
        "countries = countries.json()['Items']\n",
        "countries_proper = dict()\n",
        "for item in countries:\n",
        "  countries_proper[item['name']] = item['id'] # map NAME to ID"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5kFeONb4SjC",
        "colab_type": "text"
      },
      "source": [
        "# Clean & prepare data\n",
        "* Calculate campaign length \n",
        "* Delete all incomplete data (like country == N,0\")\n",
        "* Delete all kickstarter projects with different state than 'failed' and 'successful' \n",
        "* Cast to numerical types all non-numerical features and drop all empty data\n",
        "* Use Label Encoding or One-Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W22gDTwYsE3t",
        "colab_type": "code",
        "outputId": "a7d892da-3e0c-4250-af90-a2ce712f9da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "df_clean = df.copy()\n",
        "\n",
        "indexes = df_clean[df_clean['country'] == 'N,0\"'].index\n",
        "df_clean.drop(indexes, inplace=True)\n",
        "# drop live & undefined states\n",
        "indexes = df_clean[(df_clean['state'] == 'live') | (df_clean['state'] == 'undefined')].index\n",
        "df_clean.drop(indexes, inplace=True)\n",
        "\n",
        "df_clean['campaign_length'] = pd.to_timedelta((pd.to_datetime(df_clean['deadline']) - pd.to_datetime(df_clean['launched'])), unit='days').dt.days\n",
        "# df_clean = df_clean[(df_clean['usd_goal_real'] >= 10) & (df_clean['campaign_length'] >= 7)] # drop all with lower goal than 10$ and shorter than week\n",
        "\n",
        "##########################################################\n",
        "# \"\"\" Label Encoding - if you want to run this, just comment lines with quotation marks\n",
        "jsons = dict()\n",
        "map_dict = {\n",
        "    'category': categories_proper,\n",
        "    'main_category': main_categories_proper,\n",
        "    'country': countries_proper,\n",
        "}\n",
        "for key, val in map_dict.items():\n",
        "  df_clean[key] = df_clean[key].map(val)\n",
        "\n",
        "json.dump(jsons, open('categories.json', 'w'))\n",
        "\n",
        "df_clean.drop(['tokenized_name', 'currency', 'name'], inplace=True, axis=1)\n",
        "df_clean.dropna(inplace=True)\n",
        "# \"\"\"\n",
        "##########################################################\n",
        "\n",
        "###########################################################\n",
        "\"\"\" One-Hot Encoding - if you want to run this, just comment lines with quotation marks\n",
        "column_transformer = ColumnTransformer([('encoder', OneHotEncoder(), ['category', 'main_category', 'currency', 'country'])], sparse_threshold=0, n_jobs=-1)\n",
        "onehot = pd.DataFrame(column_transformer.fit_transform(df_clean)).set_index(df_clean.index)\n",
        "new_cols_encoding = [col.replace('encoder__x0_', '').replace('encoder__x1_', '').replace('encoder__x2_', '').replace('encoder__x3_', '') for col in column_transformer.get_feature_names()]\n",
        "onehot.columns = new_cols_encoding\n",
        "df_clean = pd.concat([df_clean, onehot], axis=1)\n",
        "df_clean.drop(['category', 'main_category', 'currency', 'country', 'tokenized_name'], inplace=True, axis=1)\n",
        "df_clean = df_clean.loc[:,~df_clean.columns.duplicated()]\n",
        "\"\"\"\n",
        "##########################################################\n",
        "\n",
        "df_xd = df_clean[~df_clean['state'].str.contains('successful')].index\n",
        "df_clean.loc[df_clean['state'].str.contains('successful'), 'state'] = 1\n",
        "df_clean.loc[df_xd, 'state'] = 0\n",
        "df_clean['state'] = df_clean['state'].astype(int)\n",
        "\n",
        "\n",
        "df_clean"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>main_category</th>\n",
              "      <th>deadline</th>\n",
              "      <th>goal</th>\n",
              "      <th>launched</th>\n",
              "      <th>pledged</th>\n",
              "      <th>state</th>\n",
              "      <th>backers</th>\n",
              "      <th>country</th>\n",
              "      <th>usd pledged</th>\n",
              "      <th>usd_pledged_real</th>\n",
              "      <th>usd_goal_real</th>\n",
              "      <th>trend</th>\n",
              "      <th>campaign_length</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1000002330</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2015-10-09</td>\n",
              "      <td>1000.0</td>\n",
              "      <td>2015-08-11 12:12:28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1533.95</td>\n",
              "      <td>26.526034</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000003930</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-11-01</td>\n",
              "      <td>30000.0</td>\n",
              "      <td>2017-09-02 04:43:57</td>\n",
              "      <td>2421.0</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2421.0</td>\n",
              "      <td>30000.00</td>\n",
              "      <td>13.684380</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000004038</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2013-02-26</td>\n",
              "      <td>45000.0</td>\n",
              "      <td>2013-01-12 00:20:50</td>\n",
              "      <td>220.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>220.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>45000.00</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000007540</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2012-04-16</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>2012-03-17 03:24:11</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5000.00</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000011046</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2015-08-29</td>\n",
              "      <td>19500.0</td>\n",
              "      <td>2015-07-04 08:35:03</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>19500.00</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999976400</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-10-17</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>2014-09-17 02:35:30</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>50000.00</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999977640</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2011-07-19</td>\n",
              "      <td>1500.0</td>\n",
              "      <td>2011-06-22 03:35:14</td>\n",
              "      <td>155.0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>155.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>1500.00</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999986353</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-08-16</td>\n",
              "      <td>15000.0</td>\n",
              "      <td>2010-07-01 19:40:30</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15000.00</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999987933</th>\n",
              "      <td>57</td>\n",
              "      <td>12</td>\n",
              "      <td>2016-02-13</td>\n",
              "      <td>15000.0</td>\n",
              "      <td>2016-01-13 18:13:53</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>200.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>15000.00</td>\n",
              "      <td>27.666667</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999988282</th>\n",
              "      <td>74</td>\n",
              "      <td>10</td>\n",
              "      <td>2011-08-16</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>2011-07-19 09:07:47</td>\n",
              "      <td>524.0</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>524.0</td>\n",
              "      <td>524.0</td>\n",
              "      <td>2000.00</td>\n",
              "      <td>18.600000</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>372054 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            category  main_category  ...      trend  campaign_length\n",
              "ID                                   ...                            \n",
              "1000002330         0              0  ...  26.526034               58\n",
              "1000003930         1              1  ...  13.684380               59\n",
              "1000004038         1              1  ...  35.000000               44\n",
              "1000007540         2              2  ...  28.000000               29\n",
              "1000011046         3              1  ...  21.800000               55\n",
              "...              ...            ...  ...        ...              ...\n",
              "999976400          8              1  ...  19.000000               29\n",
              "999977640          1              1  ...  56.000000               26\n",
              "999986353          1              1  ...  18.000000               45\n",
              "999987933         57             12  ...  27.666667               30\n",
              "999988282         74             10  ...  18.600000               27\n",
              "\n",
              "[372054 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hV19F-C5ACG",
        "colab_type": "text"
      },
      "source": [
        "# Check features correlation\n",
        "We say features are dependant, if abs(correlation) > .5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW4UTNAPqvp1",
        "colab_type": "code",
        "outputId": "cf68b596-7e12-4133-86f9-c2cb2def9f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        }
      },
      "source": [
        "corr = df_clean.corr()\n",
        "\n",
        "plt.matshow(corr)\n",
        "plt.show()\n",
        "\n",
        "corr[(corr > .5) | (corr < -.5)]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>main_category</th>\n",
              "      <th>goal</th>\n",
              "      <th>pledged</th>\n",
              "      <th>state</th>\n",
              "      <th>backers</th>\n",
              "      <th>country</th>\n",
              "      <th>usd pledged</th>\n",
              "      <th>usd_pledged_real</th>\n",
              "      <th>usd_goal_real</th>\n",
              "      <th>trend</th>\n",
              "      <th>campaign_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>main_category</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>goal</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.941676</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pledged</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.718208</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.858153</td>\n",
              "      <td>0.953262</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>state</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>backers</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.718208</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.697767</td>\n",
              "      <td>0.753399</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>usd pledged</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.858153</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.697767</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.908014</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>usd_pledged_real</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.953262</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.753399</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.908014</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>usd_goal_real</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.941676</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trend</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>campaign_length</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  category  main_category  ...  trend  campaign_length\n",
              "category               1.0            NaN  ...    NaN              NaN\n",
              "main_category          NaN            1.0  ...    NaN              NaN\n",
              "goal                   NaN            NaN  ...    NaN              NaN\n",
              "pledged                NaN            NaN  ...    NaN              NaN\n",
              "state                  NaN            NaN  ...    NaN              NaN\n",
              "backers                NaN            NaN  ...    NaN              NaN\n",
              "country                NaN            NaN  ...    NaN              NaN\n",
              "usd pledged            NaN            NaN  ...    NaN              NaN\n",
              "usd_pledged_real       NaN            NaN  ...    NaN              NaN\n",
              "usd_goal_real          NaN            NaN  ...    NaN              NaN\n",
              "trend                  NaN            NaN  ...    1.0              NaN\n",
              "campaign_length        NaN            NaN  ...    NaN              1.0\n",
              "\n",
              "[12 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZooYFrhL5TQI",
        "colab_type": "text"
      },
      "source": [
        "# Delete unnecessary features\n",
        "We delete dupe features (like converted goal value) and the ones that user won't be able to provide, like backers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BPFR1x6xk8S",
        "colab_type": "code",
        "outputId": "281b3956-c661-42a2-ea66-be249a048561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "df_shortened = df_clean.copy()\n",
        "df_shortened.drop(['pledged', 'backers', 'usd pledged', 'deadline', 'launched', 'usd_pledged_real', 'goal'], axis=1, inplace=True)\n",
        "df_shortened"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>main_category</th>\n",
              "      <th>state</th>\n",
              "      <th>country</th>\n",
              "      <th>usd_goal_real</th>\n",
              "      <th>trend</th>\n",
              "      <th>campaign_length</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1000002330</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1533.95</td>\n",
              "      <td>26.526034</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000003930</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>30000.00</td>\n",
              "      <td>13.684380</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000004038</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>45000.00</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000007540</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5000.00</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000011046</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>19500.00</td>\n",
              "      <td>21.800000</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999976400</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>50000.00</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999977640</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1500.00</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999986353</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15000.00</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999987933</th>\n",
              "      <td>57</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15000.00</td>\n",
              "      <td>27.666667</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999988282</th>\n",
              "      <td>74</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2000.00</td>\n",
              "      <td>18.600000</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>372054 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            category  main_category  ...      trend  campaign_length\n",
              "ID                                   ...                            \n",
              "1000002330         0              0  ...  26.526034               58\n",
              "1000003930         1              1  ...  13.684380               59\n",
              "1000004038         1              1  ...  35.000000               44\n",
              "1000007540         2              2  ...  28.000000               29\n",
              "1000011046         3              1  ...  21.800000               55\n",
              "...              ...            ...  ...        ...              ...\n",
              "999976400          8              1  ...  19.000000               29\n",
              "999977640          1              1  ...  56.000000               26\n",
              "999986353          1              1  ...  18.000000               45\n",
              "999987933         57             12  ...  27.666667               30\n",
              "999988282         74             10  ...  18.600000               27\n",
              "\n",
              "[372054 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF0H2yG77AAu",
        "colab_type": "text"
      },
      "source": [
        "# Split data\n",
        "Split data for training & test set, with 10% being in test set. 30k is enough for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCbI4dsZ7Btb",
        "colab_type": "code",
        "outputId": "fec03dad-d569-4700-cb31-386530699100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "X = df_shortened.drop('state', axis=1)\n",
        "y = df_shortened['state']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=2137) # 90%:10%\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.1, random_state=2137) # 81%:9% -> 90%\n",
        "X_train"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>main_category</th>\n",
              "      <th>country</th>\n",
              "      <th>usd_goal_real</th>\n",
              "      <th>trend</th>\n",
              "      <th>campaign_length</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1942408238</th>\n",
              "      <td>121</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>10000.00</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151688513</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1680.87</td>\n",
              "      <td>18.200000</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1202127605</th>\n",
              "      <td>27</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>60000.00</td>\n",
              "      <td>33.600000</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1481031083</th>\n",
              "      <td>49</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>40000.00</td>\n",
              "      <td>38.500000</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1354614493</th>\n",
              "      <td>117</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>23266.64</td>\n",
              "      <td>36.488323</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>696627033</th>\n",
              "      <td>19</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1500.00</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259775964</th>\n",
              "      <td>46</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5000.00</td>\n",
              "      <td>13.500000</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002951756</th>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>12945.99</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>723212184</th>\n",
              "      <td>40</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1750.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020324581</th>\n",
              "      <td>36</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "      <td>5609.97</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>301363 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            category  main_category  ...      trend  campaign_length\n",
              "ID                                   ...                            \n",
              "1942408238       121              6  ...  10.000000               29\n",
              "151688513          3              1  ...  18.200000               29\n",
              "1202127605        27             11  ...  33.600000               23\n",
              "1481031083        49             12  ...  38.500000               29\n",
              "1354614493       117              9  ...  36.488323               59\n",
              "...              ...            ...  ...        ...              ...\n",
              "696627033         19              9  ...  38.000000               40\n",
              "259775964         46              1  ...  13.500000               29\n",
              "2002951756         7              4  ...  42.000000               29\n",
              "723212184         40              2  ...   0.000000               31\n",
              "1020324581        36             12  ...   6.000000               29\n",
              "\n",
              "[301363 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d5noxESaCm1",
        "colab_type": "text"
      },
      "source": [
        "# Data Over/Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdxebKr6jz-6",
        "colab_type": "code",
        "outputId": "8c66a1cf-a48f-497b-863a-aea06965abe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "print(pd.Series(train_data['y']).value_counts())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0325224e1d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy9L7KY7aCpu",
        "colab_type": "code",
        "outputId": "90d4d216-838d-48b7-83a8-41710541b366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "def sample_data(sampler, X_train, y_train, cols):\n",
        "    start = time.time()\n",
        "    X_train_new, y_train_new = sampler.fit_sample(X_train, y_train)\n",
        "    X_train_new = pd.DataFrame(X_train_new)\n",
        "    X_train_new.columns = cols\n",
        "    print(f\"SMOTENC done in {round(time.time() - start, 2)} seconds\")\n",
        "\n",
        "    return {\n",
        "      'x': X_train_new,\n",
        "      'y': y_train_new,\n",
        "    }\n",
        "\n",
        "\n",
        "train_data = sample_data(SMOTENC([0, 1, 2], n_jobs=-1), X_train, y_train, X_train.columns)\n",
        "test_data = { 'x': X_test, 'y': y_test }\n",
        "val_data = { 'x': X_val, 'y': y_val }\n",
        "\n",
        "print(pd.Series(train_data['y']).value_counts())\n",
        "print(pd.Series(test_data['y']).value_counts())\n",
        "print(pd.Series(val_data['y']).value_counts())\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SMOTENC done in 538.48 seconds\n",
            "1    192759\n",
            "0    192759\n",
            "dtype: int64\n",
            "0    23913\n",
            "1    13293\n",
            "Name: state, dtype: int64\n",
            "0    21532\n",
            "1    11953\n",
            "Name: state, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRj1v4ftIbh3",
        "colab_type": "text"
      },
      "source": [
        "# (Optional) Delete all irrelevant features\n",
        "Delete all irrelevant features, but keep AT MAX 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaB8YDCEIbnR",
        "colab_type": "code",
        "outputId": "c2611c5b-d582-48a6-ec5f-4e7cb7970923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\" If you want to use this cell, just comment lines with quotation marks at the beginning\n",
        "logistic = LogisticRegression(C=1, penalty=\"l2\", max_iter=1000).fit(X_train, y_train)\n",
        "model = SelectFromModel(logistic, prefit=True, max_features=5)\n",
        "\n",
        "X_new = model.transform(X_train)\n",
        "\n",
        "selected_features = pd.DataFrame(model.inverse_transform(X_new), index=X_train.index, columns=X_train.columns)\n",
        "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
        "\n",
        "X_train = X_train[selected_columns]\n",
        "X_test = X_test[selected_columns]\n",
        "\n",
        "selected_features\n",
        "\"\"\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' If you want to use this cell, just comment lines with quotation marks at the beginning\\nlogistic = LogisticRegression(C=1, penalty=\"l2\", max_iter=1000).fit(X_train, y_train)\\nmodel = SelectFromModel(logistic, prefit=True, max_features=5)\\n\\nX_new = model.transform(X_train)\\n\\nselected_features = pd.DataFrame(model.inverse_transform(X_new), index=X_train.index, columns=X_train.columns)\\nselected_columns = selected_features.columns[selected_features.var() != 0]\\n\\nX_train = X_train[selected_columns]\\nX_test = X_test[selected_columns]\\n\\nselected_features\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6os4POct6Ige",
        "colab_type": "text"
      },
      "source": [
        "# Standarization & min-max scaling\n",
        "Standarization -> mean-std\n",
        "\n",
        "Min-Max scaling -> min-max"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DKe4oTz0QIu",
        "colab_type": "code",
        "outputId": "c70521e0-73b5-4100-eff6-8936dc5d14f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "def standarize(X_train, X_test, X_val):\n",
        "    cols = X_train.columns\n",
        "    indexes_x_train = X_train.index\n",
        "    indexes_x_test = X_test.index\n",
        "    indexes_x_val = X_val.index\n",
        "\n",
        "    X_train_categorical = X_train[['category', 'main_category', 'country']]\n",
        "    X_test_categorical = X_test[['category', 'main_category', 'country']]\n",
        "    X_val_categorical = X_val[['category', 'main_category', 'country']]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(X_train.drop(['category', 'main_category', 'country'], axis=1))\n",
        "    \n",
        "    X_train = pd.concat([X_train_categorical, pd.DataFrame(scaler.transform(X_train.drop(['category', 'main_category', 'country'], axis=1))).set_index(indexes_x_train)], axis=1)    \n",
        "    X_test = pd.concat([X_test_categorical, pd.DataFrame(scaler.transform(X_test.drop(['category', 'main_category', 'country'], axis=1))).set_index(indexes_x_test)], axis=1)\n",
        "    X_val = pd.concat([X_val_categorical, pd.DataFrame(scaler.transform(X_val.drop(['category', 'main_category', 'country'], axis=1))).set_index(indexes_x_val)], axis=1)\n",
        "    \n",
        "    X_train.columns = cols\n",
        "    X_test.columns = cols\n",
        "    X_val.columns = cols\n",
        "\n",
        "    return X_train, X_test, X_val, scaler\n",
        "\n",
        "train_data['x'], test_data['x'], val_data['x'], standarizer = standarize(train_data['x'], test_data['x'], val_data['x'])\n",
        "test_data['x']"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>main_category</th>\n",
              "      <th>country</th>\n",
              "      <th>usd_goal_real</th>\n",
              "      <th>trend</th>\n",
              "      <th>campaign_length</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>152960477</th>\n",
              "      <td>28</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.032425</td>\n",
              "      <td>1.431441</td>\n",
              "      <td>-0.061184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>951135138</th>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.036695</td>\n",
              "      <td>0.765281</td>\n",
              "      <td>-0.061184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2057049378</th>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.034937</td>\n",
              "      <td>-1.043867</td>\n",
              "      <td>-0.045868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429002892</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.002283</td>\n",
              "      <td>-1.066388</td>\n",
              "      <td>0.168559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>655581038</th>\n",
              "      <td>46</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.030415</td>\n",
              "      <td>0.017313</td>\n",
              "      <td>-0.045868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605794243</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.012330</td>\n",
              "      <td>-0.762611</td>\n",
              "      <td>0.168559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1630168153</th>\n",
              "      <td>43</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.029411</td>\n",
              "      <td>-0.581062</td>\n",
              "      <td>-0.061184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162940962</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.027401</td>\n",
              "      <td>0.021988</td>\n",
              "      <td>-0.061184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378348256</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.031721</td>\n",
              "      <td>1.326258</td>\n",
              "      <td>0.000081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300446592</th>\n",
              "      <td>20</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.034434</td>\n",
              "      <td>-0.959721</td>\n",
              "      <td>-0.061184</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>37206 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            category  main_category  ...     trend  campaign_length\n",
              "ID                                   ...                           \n",
              "152960477         28              2  ...  1.431441        -0.061184\n",
              "951135138         30             10  ...  0.765281        -0.061184\n",
              "2057049378        32              0  ... -1.043867        -0.045868\n",
              "429002892          8              1  ... -1.066388         0.168559\n",
              "655581038         46              1  ...  0.017313        -0.045868\n",
              "...              ...            ...  ...       ...              ...\n",
              "605794243          8              1  ... -0.762611         0.168559\n",
              "1630168153        43              6  ... -0.581062        -0.061184\n",
              "162940962          5              3  ...  0.021988        -0.061184\n",
              "378348256          9              0  ...  1.326258         0.000081\n",
              "300446592         20              7  ... -0.959721        -0.061184\n",
              "\n",
              "[37206 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY0Z7bMVyKQj",
        "colab_type": "text"
      },
      "source": [
        "# Load Standarizer (Scaler) from Web Server\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGqp4CRxo6Yc",
        "colab_type": "text"
      },
      "source": [
        "#Deep Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lssBEzq15SMD",
        "colab_type": "code",
        "outputId": "21a86581-cec8-4d70-a803-f9aa60fecb5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "! pip install -q tensorflow-model-optimization"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 174kB 5.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 15.5MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiwTLTWupD1V",
        "colab_type": "code",
        "outputId": "04a5caa8-f525-42a7-ebeb-8bafa33338e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "l = tf.keras.layers\n",
        "batch_size = 1024\n",
        "epochs = 500\n",
        "end_step = np.ceil(1.0 * train_data['x'].shape[0] / batch_size).astype(np.int32) * epochs\n",
        "pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.01,\n",
        "                                                   final_sparsity=0.2,\n",
        "                                                   begin_step=round(end_step/epochs/2),\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=end_step/epochs)\n",
        "}\n",
        "tf.random.set_seed(2137)\n",
        "pruned_model = tf.keras.Sequential([\n",
        "    sparsity.prune_low_magnitude(\n",
        "        tf.keras.layers.Dense(12, input_dim=train_data['x'].shape[1], activation='selu'),\n",
        "        **pruning_params),\n",
        "    l.BatchNormalization(),\n",
        "    sparsity.prune_low_magnitude(\n",
        "        tf.keras.layers.Dense(12, activation='relu'),**pruning_params),\n",
        "    l.Flatten(),\n",
        "    sparsity.prune_low_magnitude(\n",
        "        tf.keras.layers.Dense(12*train_data['x'].shape[1], activation='selu'),**pruning_params),\n",
        "    l.Dropout(0.001),\n",
        "    sparsity.prune_low_magnitude(tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "                                 **pruning_params)\n",
        "])\n",
        "\n",
        "pruned_model.summary()\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_dense_28 (None, 12)                158       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 12)                48        \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_29 (None, 12)                302       \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 12)                0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_30 (None, 72)                1802      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 72)                0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_31 (None, 1)                 147       \n",
            "=================================================================\n",
            "Total params: 2,457\n",
            "Trainable params: 1,273\n",
            "Non-trainable params: 1,184\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsUTq71RFaLP",
        "colab_type": "code",
        "outputId": "5b7ccda8-19c4-4305-82e4-ec90fc07e3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pruned_model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy,\n",
        "    optimizer='Adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Add a pruning step callback to peg the pruning step to the optimizer's\n",
        "# step. Also add a callback to add pruning summaries to tensorboard\n",
        "callbacks = [\n",
        "    sparsity.UpdatePruningStep(),\n",
        "    sparsity.PruningSummaries(log_dir=logdir, profile_batch=0)\n",
        "]\n",
        "\n",
        "pruned_model.fit(train_data['x'],train_data['y'],\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          callbacks=callbacks,\n",
        "          validation_data=(val_data['x'],val_data['y']))\n",
        "\n",
        "score = pruned_model.evaluate(test_data['x'],test_data['y'], verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6786 - accuracy: 0.5656 - val_loss: 0.6815 - val_accuracy: 0.5534\n",
            "Epoch 2/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6643 - accuracy: 0.5942 - val_loss: 0.6605 - val_accuracy: 0.5874\n",
            "Epoch 3/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6539 - accuracy: 0.6083 - val_loss: 0.6340 - val_accuracy: 0.6334\n",
            "Epoch 4/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6469 - accuracy: 0.6168 - val_loss: 0.6423 - val_accuracy: 0.6046\n",
            "Epoch 5/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6418 - accuracy: 0.6237 - val_loss: 0.6204 - val_accuracy: 0.6485\n",
            "Epoch 6/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6379 - accuracy: 0.6295 - val_loss: 0.6365 - val_accuracy: 0.6161\n",
            "Epoch 7/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6354 - accuracy: 0.6334 - val_loss: 0.6385 - val_accuracy: 0.6147\n",
            "Epoch 8/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6340 - accuracy: 0.6356 - val_loss: 0.6491 - val_accuracy: 0.5967\n",
            "Epoch 9/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6329 - accuracy: 0.6363 - val_loss: 0.6419 - val_accuracy: 0.6079\n",
            "Epoch 10/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6322 - accuracy: 0.6367 - val_loss: 0.6580 - val_accuracy: 0.5873\n",
            "Epoch 11/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6315 - accuracy: 0.6380 - val_loss: 0.6419 - val_accuracy: 0.6101\n",
            "Epoch 12/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6307 - accuracy: 0.6389 - val_loss: 0.6265 - val_accuracy: 0.6275\n",
            "Epoch 13/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6301 - accuracy: 0.6390 - val_loss: 0.6373 - val_accuracy: 0.6132\n",
            "Epoch 14/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6300 - accuracy: 0.6400 - val_loss: 0.6237 - val_accuracy: 0.6352\n",
            "Epoch 15/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6297 - accuracy: 0.6396 - val_loss: 0.6390 - val_accuracy: 0.6171\n",
            "Epoch 16/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6296 - accuracy: 0.6404 - val_loss: 0.6372 - val_accuracy: 0.6108\n",
            "Epoch 17/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6294 - accuracy: 0.6393 - val_loss: 0.6230 - val_accuracy: 0.6354\n",
            "Epoch 18/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6289 - accuracy: 0.6410 - val_loss: 0.6330 - val_accuracy: 0.6180\n",
            "Epoch 19/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6286 - accuracy: 0.6409 - val_loss: 0.6391 - val_accuracy: 0.6092\n",
            "Epoch 20/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6286 - accuracy: 0.6409 - val_loss: 0.6529 - val_accuracy: 0.5909\n",
            "Epoch 21/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6284 - accuracy: 0.6411 - val_loss: 0.6319 - val_accuracy: 0.6185\n",
            "Epoch 22/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6284 - accuracy: 0.6411 - val_loss: 0.6430 - val_accuracy: 0.6056\n",
            "Epoch 23/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6278 - accuracy: 0.6414 - val_loss: 0.6232 - val_accuracy: 0.6344\n",
            "Epoch 24/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6276 - accuracy: 0.6417 - val_loss: 0.6311 - val_accuracy: 0.6220\n",
            "Epoch 25/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6276 - accuracy: 0.6409 - val_loss: 0.6512 - val_accuracy: 0.6001\n",
            "Epoch 26/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6276 - accuracy: 0.6417 - val_loss: 0.6298 - val_accuracy: 0.6257\n",
            "Epoch 27/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6274 - accuracy: 0.6415 - val_loss: 0.6466 - val_accuracy: 0.5996\n",
            "Epoch 28/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6271 - accuracy: 0.6416 - val_loss: 0.6296 - val_accuracy: 0.6222\n",
            "Epoch 29/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6272 - accuracy: 0.6420 - val_loss: 0.6269 - val_accuracy: 0.6297\n",
            "Epoch 30/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6270 - accuracy: 0.6414 - val_loss: 0.6512 - val_accuracy: 0.5923\n",
            "Epoch 31/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6265 - accuracy: 0.6429 - val_loss: 0.6311 - val_accuracy: 0.6200\n",
            "Epoch 32/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6266 - accuracy: 0.6424 - val_loss: 0.6366 - val_accuracy: 0.6156\n",
            "Epoch 33/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6269 - accuracy: 0.6421 - val_loss: 0.6379 - val_accuracy: 0.6147\n",
            "Epoch 34/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6263 - accuracy: 0.6422 - val_loss: 0.6195 - val_accuracy: 0.6394\n",
            "Epoch 35/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6264 - accuracy: 0.6418 - val_loss: 0.6403 - val_accuracy: 0.6087\n",
            "Epoch 36/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6264 - accuracy: 0.6426 - val_loss: 0.6353 - val_accuracy: 0.6122\n",
            "Epoch 37/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6260 - accuracy: 0.6430 - val_loss: 0.6265 - val_accuracy: 0.6311\n",
            "Epoch 38/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6259 - accuracy: 0.6430 - val_loss: 0.6393 - val_accuracy: 0.6111\n",
            "Epoch 39/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6263 - accuracy: 0.6426 - val_loss: 0.6260 - val_accuracy: 0.6288\n",
            "Epoch 40/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6256 - accuracy: 0.6426 - val_loss: 0.6233 - val_accuracy: 0.6357\n",
            "Epoch 41/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6258 - accuracy: 0.6430 - val_loss: 0.6416 - val_accuracy: 0.6082\n",
            "Epoch 42/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6254 - accuracy: 0.6439 - val_loss: 0.6359 - val_accuracy: 0.6204\n",
            "Epoch 43/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6253 - accuracy: 0.6434 - val_loss: 0.6242 - val_accuracy: 0.6316\n",
            "Epoch 44/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6251 - accuracy: 0.6442 - val_loss: 0.6424 - val_accuracy: 0.6101\n",
            "Epoch 45/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6249 - accuracy: 0.6442 - val_loss: 0.6256 - val_accuracy: 0.6311\n",
            "Epoch 46/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6252 - accuracy: 0.6435 - val_loss: 0.6373 - val_accuracy: 0.6146\n",
            "Epoch 47/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6253 - accuracy: 0.6436 - val_loss: 0.6410 - val_accuracy: 0.6154\n",
            "Epoch 48/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6251 - accuracy: 0.6439 - val_loss: 0.6345 - val_accuracy: 0.6180\n",
            "Epoch 49/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6247 - accuracy: 0.6439 - val_loss: 0.6287 - val_accuracy: 0.6289\n",
            "Epoch 50/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6247 - accuracy: 0.6445 - val_loss: 0.6446 - val_accuracy: 0.6046\n",
            "Epoch 51/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6245 - accuracy: 0.6448 - val_loss: 0.6495 - val_accuracy: 0.5987\n",
            "Epoch 52/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6249 - accuracy: 0.6434 - val_loss: 0.6300 - val_accuracy: 0.6245\n",
            "Epoch 53/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6241 - accuracy: 0.6448 - val_loss: 0.6610 - val_accuracy: 0.5907\n",
            "Epoch 54/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6244 - accuracy: 0.6444 - val_loss: 0.6191 - val_accuracy: 0.6398\n",
            "Epoch 55/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6238 - accuracy: 0.6448 - val_loss: 0.6432 - val_accuracy: 0.6099\n",
            "Epoch 56/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6241 - accuracy: 0.6450 - val_loss: 0.6252 - val_accuracy: 0.6308\n",
            "Epoch 57/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6237 - accuracy: 0.6453 - val_loss: 0.6279 - val_accuracy: 0.6277\n",
            "Epoch 58/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6238 - accuracy: 0.6455 - val_loss: 0.6332 - val_accuracy: 0.6201\n",
            "Epoch 59/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6240 - accuracy: 0.6447 - val_loss: 0.6350 - val_accuracy: 0.6208\n",
            "Epoch 60/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6235 - accuracy: 0.6454 - val_loss: 0.6217 - val_accuracy: 0.6359\n",
            "Epoch 61/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6233 - accuracy: 0.6453 - val_loss: 0.6407 - val_accuracy: 0.6128\n",
            "Epoch 62/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6235 - accuracy: 0.6453 - val_loss: 0.6283 - val_accuracy: 0.6277\n",
            "Epoch 63/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6232 - accuracy: 0.6457 - val_loss: 0.6492 - val_accuracy: 0.6008\n",
            "Epoch 64/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6234 - accuracy: 0.6460 - val_loss: 0.6307 - val_accuracy: 0.6250\n",
            "Epoch 65/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6230 - accuracy: 0.6466 - val_loss: 0.6413 - val_accuracy: 0.6107\n",
            "Epoch 66/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6231 - accuracy: 0.6452 - val_loss: 0.6216 - val_accuracy: 0.6363\n",
            "Epoch 67/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6227 - accuracy: 0.6457 - val_loss: 0.6375 - val_accuracy: 0.6149\n",
            "Epoch 68/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6226 - accuracy: 0.6465 - val_loss: 0.6319 - val_accuracy: 0.6218\n",
            "Epoch 69/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6225 - accuracy: 0.6469 - val_loss: 0.6314 - val_accuracy: 0.6208\n",
            "Epoch 70/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6226 - accuracy: 0.6461 - val_loss: 0.6342 - val_accuracy: 0.6202\n",
            "Epoch 71/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6227 - accuracy: 0.6455 - val_loss: 0.6146 - val_accuracy: 0.6525\n",
            "Epoch 72/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6221 - accuracy: 0.6470 - val_loss: 0.6416 - val_accuracy: 0.6162\n",
            "Epoch 73/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6222 - accuracy: 0.6467 - val_loss: 0.6446 - val_accuracy: 0.6071\n",
            "Epoch 74/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6220 - accuracy: 0.6472 - val_loss: 0.6395 - val_accuracy: 0.6105\n",
            "Epoch 75/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6221 - accuracy: 0.6461 - val_loss: 0.6225 - val_accuracy: 0.6374\n",
            "Epoch 76/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6219 - accuracy: 0.6462 - val_loss: 0.6320 - val_accuracy: 0.6251\n",
            "Epoch 77/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6217 - accuracy: 0.6469 - val_loss: 0.6303 - val_accuracy: 0.6235\n",
            "Epoch 78/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6217 - accuracy: 0.6459 - val_loss: 0.6260 - val_accuracy: 0.6311\n",
            "Epoch 79/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6215 - accuracy: 0.6470 - val_loss: 0.6286 - val_accuracy: 0.6249\n",
            "Epoch 80/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6211 - accuracy: 0.6477 - val_loss: 0.6165 - val_accuracy: 0.6445\n",
            "Epoch 81/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6215 - accuracy: 0.6469 - val_loss: 0.6182 - val_accuracy: 0.6404\n",
            "Epoch 82/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6210 - accuracy: 0.6477 - val_loss: 0.6232 - val_accuracy: 0.6356\n",
            "Epoch 83/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6212 - accuracy: 0.6474 - val_loss: 0.6119 - val_accuracy: 0.6511\n",
            "Epoch 84/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6208 - accuracy: 0.6481 - val_loss: 0.6308 - val_accuracy: 0.6254\n",
            "Epoch 85/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6209 - accuracy: 0.6472 - val_loss: 0.6509 - val_accuracy: 0.6022\n",
            "Epoch 86/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6206 - accuracy: 0.6483 - val_loss: 0.6305 - val_accuracy: 0.6265\n",
            "Epoch 87/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6209 - accuracy: 0.6472 - val_loss: 0.6175 - val_accuracy: 0.6440\n",
            "Epoch 88/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6209 - accuracy: 0.6480 - val_loss: 0.6314 - val_accuracy: 0.6236\n",
            "Epoch 89/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6204 - accuracy: 0.6485 - val_loss: 0.6245 - val_accuracy: 0.6341\n",
            "Epoch 90/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6206 - accuracy: 0.6480 - val_loss: 0.6476 - val_accuracy: 0.6073\n",
            "Epoch 91/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6203 - accuracy: 0.6477 - val_loss: 0.6224 - val_accuracy: 0.6405\n",
            "Epoch 92/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6204 - accuracy: 0.6481 - val_loss: 0.6174 - val_accuracy: 0.6438\n",
            "Epoch 93/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6200 - accuracy: 0.6483 - val_loss: 0.6195 - val_accuracy: 0.6417\n",
            "Epoch 94/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6202 - accuracy: 0.6484 - val_loss: 0.6174 - val_accuracy: 0.6434\n",
            "Epoch 95/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6200 - accuracy: 0.6485 - val_loss: 0.6236 - val_accuracy: 0.6343\n",
            "Epoch 96/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6198 - accuracy: 0.6483 - val_loss: 0.6321 - val_accuracy: 0.6269\n",
            "Epoch 97/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6201 - accuracy: 0.6484 - val_loss: 0.6263 - val_accuracy: 0.6308\n",
            "Epoch 98/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6196 - accuracy: 0.6487 - val_loss: 0.6541 - val_accuracy: 0.5992\n",
            "Epoch 99/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6197 - accuracy: 0.6491 - val_loss: 0.6161 - val_accuracy: 0.6448\n",
            "Epoch 100/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6195 - accuracy: 0.6487 - val_loss: 0.6339 - val_accuracy: 0.6193\n",
            "Epoch 101/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6200 - accuracy: 0.6483 - val_loss: 0.6232 - val_accuracy: 0.6369\n",
            "Epoch 102/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6191 - accuracy: 0.6498 - val_loss: 0.6190 - val_accuracy: 0.6420\n",
            "Epoch 103/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6197 - accuracy: 0.6482 - val_loss: 0.6293 - val_accuracy: 0.6227\n",
            "Epoch 104/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6192 - accuracy: 0.6490 - val_loss: 0.6210 - val_accuracy: 0.6382\n",
            "Epoch 105/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6191 - accuracy: 0.6488 - val_loss: 0.6311 - val_accuracy: 0.6264\n",
            "Epoch 106/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6190 - accuracy: 0.6485 - val_loss: 0.6311 - val_accuracy: 0.6257\n",
            "Epoch 107/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6191 - accuracy: 0.6492 - val_loss: 0.6482 - val_accuracy: 0.6037\n",
            "Epoch 108/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6190 - accuracy: 0.6492 - val_loss: 0.6345 - val_accuracy: 0.6222\n",
            "Epoch 109/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6189 - accuracy: 0.6500 - val_loss: 0.6139 - val_accuracy: 0.6480\n",
            "Epoch 110/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6191 - accuracy: 0.6496 - val_loss: 0.6149 - val_accuracy: 0.6485\n",
            "Epoch 111/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6189 - accuracy: 0.6497 - val_loss: 0.6307 - val_accuracy: 0.6251\n",
            "Epoch 112/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6187 - accuracy: 0.6498 - val_loss: 0.6152 - val_accuracy: 0.6458\n",
            "Epoch 113/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6182 - accuracy: 0.6507 - val_loss: 0.6135 - val_accuracy: 0.6509\n",
            "Epoch 114/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6186 - accuracy: 0.6499 - val_loss: 0.6107 - val_accuracy: 0.6514\n",
            "Epoch 115/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6188 - accuracy: 0.6496 - val_loss: 0.6179 - val_accuracy: 0.6439\n",
            "Epoch 116/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6185 - accuracy: 0.6502 - val_loss: 0.6226 - val_accuracy: 0.6355\n",
            "Epoch 117/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6185 - accuracy: 0.6501 - val_loss: 0.6162 - val_accuracy: 0.6442\n",
            "Epoch 118/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6188 - accuracy: 0.6499 - val_loss: 0.6088 - val_accuracy: 0.6583\n",
            "Epoch 119/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6185 - accuracy: 0.6508 - val_loss: 0.6286 - val_accuracy: 0.6279\n",
            "Epoch 120/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6183 - accuracy: 0.6504 - val_loss: 0.6304 - val_accuracy: 0.6271\n",
            "Epoch 121/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6183 - accuracy: 0.6500 - val_loss: 0.6177 - val_accuracy: 0.6465\n",
            "Epoch 122/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6179 - accuracy: 0.6504 - val_loss: 0.6311 - val_accuracy: 0.6268\n",
            "Epoch 123/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6183 - accuracy: 0.6503 - val_loss: 0.6152 - val_accuracy: 0.6478\n",
            "Epoch 124/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6181 - accuracy: 0.6508 - val_loss: 0.6203 - val_accuracy: 0.6430\n",
            "Epoch 125/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6181 - accuracy: 0.6508 - val_loss: 0.6372 - val_accuracy: 0.6188\n",
            "Epoch 126/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6180 - accuracy: 0.6498 - val_loss: 0.6227 - val_accuracy: 0.6373\n",
            "Epoch 127/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6175 - accuracy: 0.6515 - val_loss: 0.6153 - val_accuracy: 0.6497\n",
            "Epoch 128/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6175 - accuracy: 0.6512 - val_loss: 0.6192 - val_accuracy: 0.6410\n",
            "Epoch 129/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6179 - accuracy: 0.6508 - val_loss: 0.6157 - val_accuracy: 0.6440\n",
            "Epoch 130/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6176 - accuracy: 0.6523 - val_loss: 0.6231 - val_accuracy: 0.6372\n",
            "Epoch 131/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6175 - accuracy: 0.6513 - val_loss: 0.6291 - val_accuracy: 0.6283\n",
            "Epoch 132/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6179 - accuracy: 0.6506 - val_loss: 0.6214 - val_accuracy: 0.6410\n",
            "Epoch 133/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6167 - accuracy: 0.6523 - val_loss: 0.6244 - val_accuracy: 0.6362\n",
            "Epoch 134/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6169 - accuracy: 0.6522 - val_loss: 0.6291 - val_accuracy: 0.6282\n",
            "Epoch 135/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6173 - accuracy: 0.6517 - val_loss: 0.6315 - val_accuracy: 0.6231\n",
            "Epoch 136/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6168 - accuracy: 0.6520 - val_loss: 0.6345 - val_accuracy: 0.6261\n",
            "Epoch 137/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6171 - accuracy: 0.6515 - val_loss: 0.6348 - val_accuracy: 0.6218\n",
            "Epoch 138/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6167 - accuracy: 0.6523 - val_loss: 0.6168 - val_accuracy: 0.6457\n",
            "Epoch 139/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6171 - accuracy: 0.6515 - val_loss: 0.6218 - val_accuracy: 0.6370\n",
            "Epoch 140/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6169 - accuracy: 0.6517 - val_loss: 0.6387 - val_accuracy: 0.6201\n",
            "Epoch 141/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6165 - accuracy: 0.6527 - val_loss: 0.6234 - val_accuracy: 0.6383\n",
            "Epoch 142/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6168 - accuracy: 0.6520 - val_loss: 0.6268 - val_accuracy: 0.6347\n",
            "Epoch 143/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6168 - accuracy: 0.6521 - val_loss: 0.6150 - val_accuracy: 0.6481\n",
            "Epoch 144/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6164 - accuracy: 0.6528 - val_loss: 0.6408 - val_accuracy: 0.6198\n",
            "Epoch 145/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6166 - accuracy: 0.6524 - val_loss: 0.6211 - val_accuracy: 0.6391\n",
            "Epoch 146/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6164 - accuracy: 0.6521 - val_loss: 0.6104 - val_accuracy: 0.6587\n",
            "Epoch 147/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6169 - accuracy: 0.6518 - val_loss: 0.5963 - val_accuracy: 0.6745\n",
            "Epoch 148/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6164 - accuracy: 0.6524 - val_loss: 0.6221 - val_accuracy: 0.6406\n",
            "Epoch 149/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6161 - accuracy: 0.6527 - val_loss: 0.6174 - val_accuracy: 0.6445\n",
            "Epoch 150/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6158 - accuracy: 0.6534 - val_loss: 0.6191 - val_accuracy: 0.6445\n",
            "Epoch 151/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6161 - accuracy: 0.6526 - val_loss: 0.6110 - val_accuracy: 0.6542\n",
            "Epoch 152/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6166 - accuracy: 0.6521 - val_loss: 0.6106 - val_accuracy: 0.6555\n",
            "Epoch 153/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6164 - accuracy: 0.6521 - val_loss: 0.6150 - val_accuracy: 0.6503\n",
            "Epoch 154/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6161 - accuracy: 0.6530 - val_loss: 0.6161 - val_accuracy: 0.6492\n",
            "Epoch 155/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6159 - accuracy: 0.6530 - val_loss: 0.6066 - val_accuracy: 0.6605\n",
            "Epoch 156/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6157 - accuracy: 0.6538 - val_loss: 0.6227 - val_accuracy: 0.6384\n",
            "Epoch 157/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6160 - accuracy: 0.6536 - val_loss: 0.6160 - val_accuracy: 0.6487\n",
            "Epoch 158/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6155 - accuracy: 0.6539 - val_loss: 0.6184 - val_accuracy: 0.6439\n",
            "Epoch 159/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6157 - accuracy: 0.6533 - val_loss: 0.6186 - val_accuracy: 0.6453\n",
            "Epoch 160/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6161 - accuracy: 0.6521 - val_loss: 0.6241 - val_accuracy: 0.6396\n",
            "Epoch 161/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6154 - accuracy: 0.6538 - val_loss: 0.6135 - val_accuracy: 0.6513\n",
            "Epoch 162/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6156 - accuracy: 0.6532 - val_loss: 0.6272 - val_accuracy: 0.6348\n",
            "Epoch 163/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6154 - accuracy: 0.6538 - val_loss: 0.6114 - val_accuracy: 0.6518\n",
            "Epoch 164/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6149 - accuracy: 0.6545 - val_loss: 0.6046 - val_accuracy: 0.6658\n",
            "Epoch 165/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6157 - accuracy: 0.6530 - val_loss: 0.6352 - val_accuracy: 0.6213\n",
            "Epoch 166/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6152 - accuracy: 0.6543 - val_loss: 0.6275 - val_accuracy: 0.6311\n",
            "Epoch 167/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6155 - accuracy: 0.6541 - val_loss: 0.6245 - val_accuracy: 0.6384\n",
            "Epoch 168/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6155 - accuracy: 0.6541 - val_loss: 0.6321 - val_accuracy: 0.6254\n",
            "Epoch 169/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6151 - accuracy: 0.6540 - val_loss: 0.6134 - val_accuracy: 0.6500\n",
            "Epoch 170/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6155 - accuracy: 0.6536 - val_loss: 0.6344 - val_accuracy: 0.6240\n",
            "Epoch 171/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6154 - accuracy: 0.6534 - val_loss: 0.6171 - val_accuracy: 0.6470\n",
            "Epoch 172/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6157 - accuracy: 0.6537 - val_loss: 0.6257 - val_accuracy: 0.6359\n",
            "Epoch 173/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6150 - accuracy: 0.6545 - val_loss: 0.6356 - val_accuracy: 0.6265\n",
            "Epoch 174/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6151 - accuracy: 0.6540 - val_loss: 0.6207 - val_accuracy: 0.6429\n",
            "Epoch 175/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6149 - accuracy: 0.6538 - val_loss: 0.6270 - val_accuracy: 0.6332\n",
            "Epoch 176/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6152 - accuracy: 0.6543 - val_loss: 0.6073 - val_accuracy: 0.6603\n",
            "Epoch 177/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6152 - accuracy: 0.6536 - val_loss: 0.6192 - val_accuracy: 0.6434\n",
            "Epoch 178/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6148 - accuracy: 0.6546 - val_loss: 0.6207 - val_accuracy: 0.6422\n",
            "Epoch 179/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6152 - accuracy: 0.6537 - val_loss: 0.6105 - val_accuracy: 0.6538\n",
            "Epoch 180/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6143 - accuracy: 0.6552 - val_loss: 0.6158 - val_accuracy: 0.6481\n",
            "Epoch 181/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6145 - accuracy: 0.6548 - val_loss: 0.6191 - val_accuracy: 0.6432\n",
            "Epoch 182/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6148 - accuracy: 0.6539 - val_loss: 0.6122 - val_accuracy: 0.6537\n",
            "Epoch 183/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6146 - accuracy: 0.6549 - val_loss: 0.6223 - val_accuracy: 0.6383\n",
            "Epoch 184/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6148 - accuracy: 0.6541 - val_loss: 0.6416 - val_accuracy: 0.6154\n",
            "Epoch 185/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6146 - accuracy: 0.6548 - val_loss: 0.6256 - val_accuracy: 0.6323\n",
            "Epoch 186/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6146 - accuracy: 0.6547 - val_loss: 0.6358 - val_accuracy: 0.6271\n",
            "Epoch 187/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6143 - accuracy: 0.6553 - val_loss: 0.6135 - val_accuracy: 0.6502\n",
            "Epoch 188/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6145 - accuracy: 0.6542 - val_loss: 0.6015 - val_accuracy: 0.6675\n",
            "Epoch 189/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6147 - accuracy: 0.6540 - val_loss: 0.6190 - val_accuracy: 0.6405\n",
            "Epoch 190/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6144 - accuracy: 0.6550 - val_loss: 0.6311 - val_accuracy: 0.6304\n",
            "Epoch 191/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6144 - accuracy: 0.6548 - val_loss: 0.6290 - val_accuracy: 0.6299\n",
            "Epoch 192/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6142 - accuracy: 0.6552 - val_loss: 0.6285 - val_accuracy: 0.6313\n",
            "Epoch 193/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6144 - accuracy: 0.6547 - val_loss: 0.6182 - val_accuracy: 0.6445\n",
            "Epoch 194/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6142 - accuracy: 0.6549 - val_loss: 0.6300 - val_accuracy: 0.6306\n",
            "Epoch 195/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6141 - accuracy: 0.6554 - val_loss: 0.6303 - val_accuracy: 0.6304\n",
            "Epoch 196/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6142 - accuracy: 0.6550 - val_loss: 0.6317 - val_accuracy: 0.6280\n",
            "Epoch 197/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6138 - accuracy: 0.6550 - val_loss: 0.6290 - val_accuracy: 0.6294\n",
            "Epoch 198/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6140 - accuracy: 0.6550 - val_loss: 0.6165 - val_accuracy: 0.6456\n",
            "Epoch 199/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6142 - accuracy: 0.6558 - val_loss: 0.6151 - val_accuracy: 0.6463\n",
            "Epoch 200/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6144 - accuracy: 0.6550 - val_loss: 0.6135 - val_accuracy: 0.6485\n",
            "Epoch 201/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6137 - accuracy: 0.6555 - val_loss: 0.6190 - val_accuracy: 0.6434\n",
            "Epoch 202/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6142 - accuracy: 0.6550 - val_loss: 0.6159 - val_accuracy: 0.6470\n",
            "Epoch 203/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6140 - accuracy: 0.6553 - val_loss: 0.6098 - val_accuracy: 0.6544\n",
            "Epoch 204/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6139 - accuracy: 0.6557 - val_loss: 0.6121 - val_accuracy: 0.6525\n",
            "Epoch 205/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6139 - accuracy: 0.6547 - val_loss: 0.6347 - val_accuracy: 0.6238\n",
            "Epoch 206/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6139 - accuracy: 0.6554 - val_loss: 0.6190 - val_accuracy: 0.6435\n",
            "Epoch 207/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6138 - accuracy: 0.6559 - val_loss: 0.6110 - val_accuracy: 0.6535\n",
            "Epoch 208/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6138 - accuracy: 0.6553 - val_loss: 0.6184 - val_accuracy: 0.6435\n",
            "Epoch 209/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6135 - accuracy: 0.6557 - val_loss: 0.6262 - val_accuracy: 0.6340\n",
            "Epoch 210/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6137 - accuracy: 0.6559 - val_loss: 0.6171 - val_accuracy: 0.6454\n",
            "Epoch 211/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6134 - accuracy: 0.6560 - val_loss: 0.6212 - val_accuracy: 0.6404\n",
            "Epoch 212/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6135 - accuracy: 0.6557 - val_loss: 0.6217 - val_accuracy: 0.6406\n",
            "Epoch 213/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6134 - accuracy: 0.6558 - val_loss: 0.6069 - val_accuracy: 0.6619\n",
            "Epoch 214/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6133 - accuracy: 0.6565 - val_loss: 0.6159 - val_accuracy: 0.6470\n",
            "Epoch 215/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6136 - accuracy: 0.6559 - val_loss: 0.6397 - val_accuracy: 0.6204\n",
            "Epoch 216/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6135 - accuracy: 0.6558 - val_loss: 0.6120 - val_accuracy: 0.6543\n",
            "Epoch 217/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6135 - accuracy: 0.6561 - val_loss: 0.6050 - val_accuracy: 0.6625\n",
            "Epoch 218/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6133 - accuracy: 0.6555 - val_loss: 0.6091 - val_accuracy: 0.6547\n",
            "Epoch 219/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6139 - accuracy: 0.6556 - val_loss: 0.6119 - val_accuracy: 0.6550\n",
            "Epoch 220/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6131 - accuracy: 0.6565 - val_loss: 0.6446 - val_accuracy: 0.6131\n",
            "Epoch 221/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6131 - accuracy: 0.6557 - val_loss: 0.6156 - val_accuracy: 0.6483\n",
            "Epoch 222/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6134 - accuracy: 0.6564 - val_loss: 0.6236 - val_accuracy: 0.6372\n",
            "Epoch 223/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6133 - accuracy: 0.6556 - val_loss: 0.6204 - val_accuracy: 0.6414\n",
            "Epoch 224/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6134 - accuracy: 0.6557 - val_loss: 0.6169 - val_accuracy: 0.6462\n",
            "Epoch 225/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6135 - accuracy: 0.6553 - val_loss: 0.6153 - val_accuracy: 0.6487\n",
            "Epoch 226/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6134 - accuracy: 0.6559 - val_loss: 0.6141 - val_accuracy: 0.6498\n",
            "Epoch 227/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6130 - accuracy: 0.6562 - val_loss: 0.6213 - val_accuracy: 0.6425\n",
            "Epoch 228/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6130 - accuracy: 0.6561 - val_loss: 0.6175 - val_accuracy: 0.6463\n",
            "Epoch 229/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6130 - accuracy: 0.6560 - val_loss: 0.5974 - val_accuracy: 0.6742\n",
            "Epoch 230/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6130 - accuracy: 0.6566 - val_loss: 0.6182 - val_accuracy: 0.6412\n",
            "Epoch 231/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6135 - accuracy: 0.6553 - val_loss: 0.6176 - val_accuracy: 0.6456\n",
            "Epoch 232/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6129 - accuracy: 0.6566 - val_loss: 0.6174 - val_accuracy: 0.6428\n",
            "Epoch 233/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6130 - accuracy: 0.6567 - val_loss: 0.6372 - val_accuracy: 0.6200\n",
            "Epoch 234/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6135 - accuracy: 0.6560 - val_loss: 0.6159 - val_accuracy: 0.6453\n",
            "Epoch 235/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6124 - accuracy: 0.6573 - val_loss: 0.6234 - val_accuracy: 0.6349\n",
            "Epoch 236/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6127 - accuracy: 0.6566 - val_loss: 0.6188 - val_accuracy: 0.6437\n",
            "Epoch 237/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6123 - accuracy: 0.6575 - val_loss: 0.6385 - val_accuracy: 0.6209\n",
            "Epoch 238/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6128 - accuracy: 0.6571 - val_loss: 0.6218 - val_accuracy: 0.6406\n",
            "Epoch 239/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6128 - accuracy: 0.6557 - val_loss: 0.6171 - val_accuracy: 0.6449\n",
            "Epoch 240/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6128 - accuracy: 0.6563 - val_loss: 0.6124 - val_accuracy: 0.6528\n",
            "Epoch 241/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6134 - accuracy: 0.6557 - val_loss: 0.6134 - val_accuracy: 0.6500\n",
            "Epoch 242/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6127 - accuracy: 0.6564 - val_loss: 0.6118 - val_accuracy: 0.6521\n",
            "Epoch 243/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6128 - accuracy: 0.6560 - val_loss: 0.6060 - val_accuracy: 0.6611\n",
            "Epoch 244/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6128 - accuracy: 0.6566 - val_loss: 0.6341 - val_accuracy: 0.6283\n",
            "Epoch 245/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6126 - accuracy: 0.6567 - val_loss: 0.6070 - val_accuracy: 0.6581\n",
            "Epoch 246/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6126 - accuracy: 0.6572 - val_loss: 0.6194 - val_accuracy: 0.6448\n",
            "Epoch 247/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6126 - accuracy: 0.6571 - val_loss: 0.6246 - val_accuracy: 0.6356\n",
            "Epoch 248/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6126 - accuracy: 0.6562 - val_loss: 0.6064 - val_accuracy: 0.6607\n",
            "Epoch 249/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6128 - accuracy: 0.6570 - val_loss: 0.6254 - val_accuracy: 0.6369\n",
            "Epoch 250/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6127 - accuracy: 0.6561 - val_loss: 0.6193 - val_accuracy: 0.6429\n",
            "Epoch 251/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6131 - accuracy: 0.6560 - val_loss: 0.6149 - val_accuracy: 0.6495\n",
            "Epoch 252/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6122 - accuracy: 0.6571 - val_loss: 0.6275 - val_accuracy: 0.6307\n",
            "Epoch 253/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6124 - accuracy: 0.6566 - val_loss: 0.6144 - val_accuracy: 0.6492\n",
            "Epoch 254/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6124 - accuracy: 0.6572 - val_loss: 0.6242 - val_accuracy: 0.6391\n",
            "Epoch 255/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6127 - accuracy: 0.6568 - val_loss: 0.6166 - val_accuracy: 0.6461\n",
            "Epoch 256/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6128 - accuracy: 0.6563 - val_loss: 0.6391 - val_accuracy: 0.6174\n",
            "Epoch 257/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6124 - accuracy: 0.6569 - val_loss: 0.6204 - val_accuracy: 0.6395\n",
            "Epoch 258/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6129 - accuracy: 0.6567 - val_loss: 0.6022 - val_accuracy: 0.6647\n",
            "Epoch 259/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6125 - accuracy: 0.6567 - val_loss: 0.6155 - val_accuracy: 0.6492\n",
            "Epoch 260/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6123 - accuracy: 0.6575 - val_loss: 0.6233 - val_accuracy: 0.6407\n",
            "Epoch 261/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6124 - accuracy: 0.6576 - val_loss: 0.6169 - val_accuracy: 0.6481\n",
            "Epoch 262/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6125 - accuracy: 0.6565 - val_loss: 0.6266 - val_accuracy: 0.6315\n",
            "Epoch 263/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6121 - accuracy: 0.6575 - val_loss: 0.6379 - val_accuracy: 0.6236\n",
            "Epoch 264/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6121 - accuracy: 0.6573 - val_loss: 0.6201 - val_accuracy: 0.6418\n",
            "Epoch 265/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6123 - accuracy: 0.6572 - val_loss: 0.6202 - val_accuracy: 0.6409\n",
            "Epoch 266/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6124 - accuracy: 0.6571 - val_loss: 0.6122 - val_accuracy: 0.6515\n",
            "Epoch 267/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6125 - accuracy: 0.6569 - val_loss: 0.6105 - val_accuracy: 0.6541\n",
            "Epoch 268/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6125 - accuracy: 0.6568 - val_loss: 0.6093 - val_accuracy: 0.6572\n",
            "Epoch 269/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6124 - accuracy: 0.6574 - val_loss: 0.6067 - val_accuracy: 0.6606\n",
            "Epoch 270/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6122 - accuracy: 0.6575 - val_loss: 0.6262 - val_accuracy: 0.6326\n",
            "Epoch 271/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6123 - accuracy: 0.6578 - val_loss: 0.6184 - val_accuracy: 0.6440\n",
            "Epoch 272/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6120 - accuracy: 0.6572 - val_loss: 0.6306 - val_accuracy: 0.6287\n",
            "Epoch 273/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6120 - accuracy: 0.6579 - val_loss: 0.6171 - val_accuracy: 0.6432\n",
            "Epoch 274/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6118 - accuracy: 0.6581 - val_loss: 0.6178 - val_accuracy: 0.6421\n",
            "Epoch 275/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6123 - accuracy: 0.6572 - val_loss: 0.6193 - val_accuracy: 0.6456\n",
            "Epoch 276/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6119 - accuracy: 0.6581 - val_loss: 0.6253 - val_accuracy: 0.6371\n",
            "Epoch 277/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6123 - accuracy: 0.6572 - val_loss: 0.6077 - val_accuracy: 0.6585\n",
            "Epoch 278/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6119 - accuracy: 0.6576 - val_loss: 0.6248 - val_accuracy: 0.6357\n",
            "Epoch 279/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6121 - accuracy: 0.6575 - val_loss: 0.6194 - val_accuracy: 0.6423\n",
            "Epoch 280/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6121 - accuracy: 0.6578 - val_loss: 0.6194 - val_accuracy: 0.6423\n",
            "Epoch 281/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6115 - accuracy: 0.6587 - val_loss: 0.6219 - val_accuracy: 0.6427\n",
            "Epoch 282/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6125 - accuracy: 0.6563 - val_loss: 0.6107 - val_accuracy: 0.6558\n",
            "Epoch 283/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6117 - accuracy: 0.6577 - val_loss: 0.6158 - val_accuracy: 0.6470\n",
            "Epoch 284/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6120 - accuracy: 0.6572 - val_loss: 0.6269 - val_accuracy: 0.6312\n",
            "Epoch 285/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6119 - accuracy: 0.6574 - val_loss: 0.6114 - val_accuracy: 0.6575\n",
            "Epoch 286/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6116 - accuracy: 0.6580 - val_loss: 0.6195 - val_accuracy: 0.6443\n",
            "Epoch 287/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6119 - accuracy: 0.6571 - val_loss: 0.6242 - val_accuracy: 0.6391\n",
            "Epoch 288/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6119 - accuracy: 0.6570 - val_loss: 0.6081 - val_accuracy: 0.6573\n",
            "Epoch 289/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6114 - accuracy: 0.6585 - val_loss: 0.6306 - val_accuracy: 0.6287\n",
            "Epoch 290/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6118 - accuracy: 0.6577 - val_loss: 0.6231 - val_accuracy: 0.6390\n",
            "Epoch 291/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6118 - accuracy: 0.6573 - val_loss: 0.6175 - val_accuracy: 0.6445\n",
            "Epoch 292/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6122 - accuracy: 0.6578 - val_loss: 0.6210 - val_accuracy: 0.6403\n",
            "Epoch 293/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6115 - accuracy: 0.6580 - val_loss: 0.6050 - val_accuracy: 0.6617\n",
            "Epoch 294/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6120 - accuracy: 0.6582 - val_loss: 0.6137 - val_accuracy: 0.6511\n",
            "Epoch 295/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6116 - accuracy: 0.6581 - val_loss: 0.6254 - val_accuracy: 0.6360\n",
            "Epoch 296/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6116 - accuracy: 0.6583 - val_loss: 0.6361 - val_accuracy: 0.6264\n",
            "Epoch 297/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6120 - accuracy: 0.6573 - val_loss: 0.6361 - val_accuracy: 0.6238\n",
            "Epoch 298/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6113 - accuracy: 0.6583 - val_loss: 0.6198 - val_accuracy: 0.6452\n",
            "Epoch 299/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6118 - accuracy: 0.6578 - val_loss: 0.6173 - val_accuracy: 0.6486\n",
            "Epoch 300/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6117 - accuracy: 0.6579 - val_loss: 0.6157 - val_accuracy: 0.6488\n",
            "Epoch 301/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6115 - accuracy: 0.6583 - val_loss: 0.6232 - val_accuracy: 0.6388\n",
            "Epoch 302/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6118 - accuracy: 0.6573 - val_loss: 0.6168 - val_accuracy: 0.6470\n",
            "Epoch 303/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6110 - accuracy: 0.6584 - val_loss: 0.6091 - val_accuracy: 0.6558\n",
            "Epoch 304/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6114 - accuracy: 0.6580 - val_loss: 0.6244 - val_accuracy: 0.6376\n",
            "Epoch 305/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6115 - accuracy: 0.6581 - val_loss: 0.6455 - val_accuracy: 0.6117\n",
            "Epoch 306/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6118 - accuracy: 0.6584 - val_loss: 0.6123 - val_accuracy: 0.6524\n",
            "Epoch 307/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6115 - accuracy: 0.6579 - val_loss: 0.6277 - val_accuracy: 0.6282\n",
            "Epoch 308/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6112 - accuracy: 0.6581 - val_loss: 0.6097 - val_accuracy: 0.6572\n",
            "Epoch 309/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6114 - accuracy: 0.6583 - val_loss: 0.6171 - val_accuracy: 0.6467\n",
            "Epoch 310/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6113 - accuracy: 0.6587 - val_loss: 0.6137 - val_accuracy: 0.6496\n",
            "Epoch 311/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6116 - accuracy: 0.6574 - val_loss: 0.6059 - val_accuracy: 0.6610\n",
            "Epoch 312/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6116 - accuracy: 0.6580 - val_loss: 0.6017 - val_accuracy: 0.6663\n",
            "Epoch 313/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6115 - accuracy: 0.6586 - val_loss: 0.6035 - val_accuracy: 0.6631\n",
            "Epoch 314/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6115 - accuracy: 0.6579 - val_loss: 0.6081 - val_accuracy: 0.6581\n",
            "Epoch 315/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6112 - accuracy: 0.6584 - val_loss: 0.6106 - val_accuracy: 0.6542\n",
            "Epoch 316/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6116 - accuracy: 0.6578 - val_loss: 0.6170 - val_accuracy: 0.6453\n",
            "Epoch 317/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6117 - accuracy: 0.6581 - val_loss: 0.6042 - val_accuracy: 0.6620\n",
            "Epoch 318/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6114 - accuracy: 0.6580 - val_loss: 0.6099 - val_accuracy: 0.6549\n",
            "Epoch 319/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6116 - accuracy: 0.6580 - val_loss: 0.6156 - val_accuracy: 0.6495\n",
            "Epoch 320/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6113 - accuracy: 0.6582 - val_loss: 0.6250 - val_accuracy: 0.6384\n",
            "Epoch 321/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6114 - accuracy: 0.6583 - val_loss: 0.6046 - val_accuracy: 0.6633\n",
            "Epoch 322/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6113 - accuracy: 0.6584 - val_loss: 0.6174 - val_accuracy: 0.6508\n",
            "Epoch 323/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6112 - accuracy: 0.6580 - val_loss: 0.6212 - val_accuracy: 0.6458\n",
            "Epoch 324/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6111 - accuracy: 0.6585 - val_loss: 0.6138 - val_accuracy: 0.6493\n",
            "Epoch 325/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6110 - accuracy: 0.6589 - val_loss: 0.6110 - val_accuracy: 0.6526\n",
            "Epoch 326/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6117 - accuracy: 0.6579 - val_loss: 0.6251 - val_accuracy: 0.6366\n",
            "Epoch 327/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6112 - accuracy: 0.6589 - val_loss: 0.6250 - val_accuracy: 0.6375\n",
            "Epoch 328/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6114 - accuracy: 0.6573 - val_loss: 0.6118 - val_accuracy: 0.6546\n",
            "Epoch 329/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6116 - accuracy: 0.6582 - val_loss: 0.6147 - val_accuracy: 0.6493\n",
            "Epoch 330/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6110 - accuracy: 0.6592 - val_loss: 0.6161 - val_accuracy: 0.6451\n",
            "Epoch 331/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6109 - accuracy: 0.6588 - val_loss: 0.6198 - val_accuracy: 0.6433\n",
            "Epoch 332/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6115 - accuracy: 0.6575 - val_loss: 0.6127 - val_accuracy: 0.6503\n",
            "Epoch 333/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6111 - accuracy: 0.6583 - val_loss: 0.6201 - val_accuracy: 0.6429\n",
            "Epoch 334/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6114 - accuracy: 0.6576 - val_loss: 0.6020 - val_accuracy: 0.6676\n",
            "Epoch 335/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6114 - accuracy: 0.6575 - val_loss: 0.6106 - val_accuracy: 0.6539\n",
            "Epoch 336/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6108 - accuracy: 0.6592 - val_loss: 0.6259 - val_accuracy: 0.6334\n",
            "Epoch 337/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6112 - accuracy: 0.6582 - val_loss: 0.6134 - val_accuracy: 0.6511\n",
            "Epoch 338/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6111 - accuracy: 0.6584 - val_loss: 0.6254 - val_accuracy: 0.6409\n",
            "Epoch 339/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6110 - accuracy: 0.6594 - val_loss: 0.6116 - val_accuracy: 0.6536\n",
            "Epoch 340/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6112 - accuracy: 0.6590 - val_loss: 0.6206 - val_accuracy: 0.6416\n",
            "Epoch 341/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6105 - accuracy: 0.6601 - val_loss: 0.6049 - val_accuracy: 0.6617\n",
            "Epoch 342/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6108 - accuracy: 0.6586 - val_loss: 0.6185 - val_accuracy: 0.6443\n",
            "Epoch 343/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6105 - accuracy: 0.6592 - val_loss: 0.6331 - val_accuracy: 0.6293\n",
            "Epoch 344/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6106 - accuracy: 0.6594 - val_loss: 0.6203 - val_accuracy: 0.6395\n",
            "Epoch 345/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6108 - accuracy: 0.6589 - val_loss: 0.6152 - val_accuracy: 0.6511\n",
            "Epoch 346/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6111 - accuracy: 0.6589 - val_loss: 0.6234 - val_accuracy: 0.6401\n",
            "Epoch 347/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6110 - accuracy: 0.6590 - val_loss: 0.6364 - val_accuracy: 0.6291\n",
            "Epoch 348/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6110 - accuracy: 0.6589 - val_loss: 0.6125 - val_accuracy: 0.6545\n",
            "Epoch 349/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6111 - accuracy: 0.6590 - val_loss: 0.6213 - val_accuracy: 0.6408\n",
            "Epoch 350/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6110 - accuracy: 0.6592 - val_loss: 0.6133 - val_accuracy: 0.6533\n",
            "Epoch 351/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6110 - accuracy: 0.6586 - val_loss: 0.6236 - val_accuracy: 0.6370\n",
            "Epoch 352/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6106 - accuracy: 0.6588 - val_loss: 0.6355 - val_accuracy: 0.6227\n",
            "Epoch 353/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6108 - accuracy: 0.6589 - val_loss: 0.6139 - val_accuracy: 0.6491\n",
            "Epoch 354/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6110 - accuracy: 0.6594 - val_loss: 0.6129 - val_accuracy: 0.6530\n",
            "Epoch 355/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6111 - accuracy: 0.6584 - val_loss: 0.6096 - val_accuracy: 0.6547\n",
            "Epoch 356/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6106 - accuracy: 0.6594 - val_loss: 0.6181 - val_accuracy: 0.6455\n",
            "Epoch 357/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6107 - accuracy: 0.6592 - val_loss: 0.6345 - val_accuracy: 0.6272\n",
            "Epoch 358/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6107 - accuracy: 0.6592 - val_loss: 0.6164 - val_accuracy: 0.6469\n",
            "Epoch 359/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6104 - accuracy: 0.6584 - val_loss: 0.6036 - val_accuracy: 0.6623\n",
            "Epoch 360/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6109 - accuracy: 0.6588 - val_loss: 0.6160 - val_accuracy: 0.6464\n",
            "Epoch 361/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6105 - accuracy: 0.6598 - val_loss: 0.6174 - val_accuracy: 0.6459\n",
            "Epoch 362/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6103 - accuracy: 0.6593 - val_loss: 0.6117 - val_accuracy: 0.6526\n",
            "Epoch 363/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6113 - accuracy: 0.6585 - val_loss: 0.6259 - val_accuracy: 0.6374\n",
            "Epoch 364/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6110 - accuracy: 0.6587 - val_loss: 0.6051 - val_accuracy: 0.6623\n",
            "Epoch 365/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6108 - accuracy: 0.6596 - val_loss: 0.6127 - val_accuracy: 0.6516\n",
            "Epoch 366/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6108 - accuracy: 0.6593 - val_loss: 0.6235 - val_accuracy: 0.6376\n",
            "Epoch 367/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6103 - accuracy: 0.6591 - val_loss: 0.6221 - val_accuracy: 0.6383\n",
            "Epoch 368/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6113 - accuracy: 0.6585 - val_loss: 0.6241 - val_accuracy: 0.6443\n",
            "Epoch 369/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6108 - accuracy: 0.6585 - val_loss: 0.6038 - val_accuracy: 0.6628\n",
            "Epoch 370/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6105 - accuracy: 0.6593 - val_loss: 0.6181 - val_accuracy: 0.6467\n",
            "Epoch 371/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6107 - accuracy: 0.6586 - val_loss: 0.6112 - val_accuracy: 0.6521\n",
            "Epoch 372/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6103 - accuracy: 0.6598 - val_loss: 0.6204 - val_accuracy: 0.6427\n",
            "Epoch 373/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6105 - accuracy: 0.6598 - val_loss: 0.6118 - val_accuracy: 0.6501\n",
            "Epoch 374/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6107 - accuracy: 0.6596 - val_loss: 0.6215 - val_accuracy: 0.6409\n",
            "Epoch 375/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6106 - accuracy: 0.6587 - val_loss: 0.6169 - val_accuracy: 0.6478\n",
            "Epoch 376/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6109 - accuracy: 0.6585 - val_loss: 0.6279 - val_accuracy: 0.6325\n",
            "Epoch 377/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6110 - accuracy: 0.6586 - val_loss: 0.6283 - val_accuracy: 0.6314\n",
            "Epoch 378/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6104 - accuracy: 0.6593 - val_loss: 0.6150 - val_accuracy: 0.6507\n",
            "Epoch 379/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6103 - accuracy: 0.6593 - val_loss: 0.6167 - val_accuracy: 0.6440\n",
            "Epoch 380/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6108 - accuracy: 0.6588 - val_loss: 0.6207 - val_accuracy: 0.6458\n",
            "Epoch 381/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6104 - accuracy: 0.6597 - val_loss: 0.6090 - val_accuracy: 0.6572\n",
            "Epoch 382/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6106 - accuracy: 0.6585 - val_loss: 0.6190 - val_accuracy: 0.6447\n",
            "Epoch 383/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6108 - accuracy: 0.6587 - val_loss: 0.6185 - val_accuracy: 0.6426\n",
            "Epoch 384/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6102 - accuracy: 0.6598 - val_loss: 0.6048 - val_accuracy: 0.6639\n",
            "Epoch 385/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6102 - accuracy: 0.6598 - val_loss: 0.6156 - val_accuracy: 0.6492\n",
            "Epoch 386/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6105 - accuracy: 0.6591 - val_loss: 0.6115 - val_accuracy: 0.6532\n",
            "Epoch 387/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6107 - accuracy: 0.6593 - val_loss: 0.6121 - val_accuracy: 0.6521\n",
            "Epoch 388/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6105 - accuracy: 0.6592 - val_loss: 0.6197 - val_accuracy: 0.6397\n",
            "Epoch 389/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6109 - accuracy: 0.6589 - val_loss: 0.6029 - val_accuracy: 0.6627\n",
            "Epoch 390/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6102 - accuracy: 0.6601 - val_loss: 0.6121 - val_accuracy: 0.6552\n",
            "Epoch 391/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6106 - accuracy: 0.6588 - val_loss: 0.6099 - val_accuracy: 0.6578\n",
            "Epoch 392/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6106 - accuracy: 0.6591 - val_loss: 0.6078 - val_accuracy: 0.6599\n",
            "Epoch 393/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6108 - accuracy: 0.6586 - val_loss: 0.6388 - val_accuracy: 0.6227\n",
            "Epoch 394/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6105 - accuracy: 0.6589 - val_loss: 0.6190 - val_accuracy: 0.6449\n",
            "Epoch 395/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6107 - accuracy: 0.6592 - val_loss: 0.6163 - val_accuracy: 0.6483\n",
            "Epoch 396/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6105 - accuracy: 0.6597 - val_loss: 0.6032 - val_accuracy: 0.6628\n",
            "Epoch 397/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6104 - accuracy: 0.6594 - val_loss: 0.6108 - val_accuracy: 0.6516\n",
            "Epoch 398/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6105 - accuracy: 0.6591 - val_loss: 0.6369 - val_accuracy: 0.6239\n",
            "Epoch 399/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6104 - accuracy: 0.6590 - val_loss: 0.6129 - val_accuracy: 0.6514\n",
            "Epoch 400/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6101 - accuracy: 0.6598 - val_loss: 0.6162 - val_accuracy: 0.6447\n",
            "Epoch 401/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6103 - accuracy: 0.6590 - val_loss: 0.6074 - val_accuracy: 0.6613\n",
            "Epoch 402/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6108 - accuracy: 0.6591 - val_loss: 0.6097 - val_accuracy: 0.6576\n",
            "Epoch 403/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6106 - accuracy: 0.6589 - val_loss: 0.6175 - val_accuracy: 0.6433\n",
            "Epoch 404/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6105 - accuracy: 0.6596 - val_loss: 0.6157 - val_accuracy: 0.6511\n",
            "Epoch 405/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6104 - accuracy: 0.6589 - val_loss: 0.6226 - val_accuracy: 0.6432\n",
            "Epoch 406/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6102 - accuracy: 0.6602 - val_loss: 0.6089 - val_accuracy: 0.6567\n",
            "Epoch 407/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6104 - accuracy: 0.6601 - val_loss: 0.6300 - val_accuracy: 0.6310\n",
            "Epoch 408/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6105 - accuracy: 0.6595 - val_loss: 0.6068 - val_accuracy: 0.6586\n",
            "Epoch 409/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6101 - accuracy: 0.6596 - val_loss: 0.6121 - val_accuracy: 0.6511\n",
            "Epoch 410/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6105 - accuracy: 0.6595 - val_loss: 0.6243 - val_accuracy: 0.6357\n",
            "Epoch 411/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6102 - accuracy: 0.6599 - val_loss: 0.6241 - val_accuracy: 0.6395\n",
            "Epoch 412/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6101 - accuracy: 0.6598 - val_loss: 0.6177 - val_accuracy: 0.6464\n",
            "Epoch 413/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6106 - accuracy: 0.6598 - val_loss: 0.6099 - val_accuracy: 0.6564\n",
            "Epoch 414/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6103 - accuracy: 0.6595 - val_loss: 0.6138 - val_accuracy: 0.6500\n",
            "Epoch 415/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6103 - accuracy: 0.6591 - val_loss: 0.6070 - val_accuracy: 0.6602\n",
            "Epoch 416/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6109 - accuracy: 0.6599 - val_loss: 0.6133 - val_accuracy: 0.6538\n",
            "Epoch 417/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6101 - accuracy: 0.6600 - val_loss: 0.6117 - val_accuracy: 0.6532\n",
            "Epoch 418/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6103 - accuracy: 0.6599 - val_loss: 0.6054 - val_accuracy: 0.6601\n",
            "Epoch 419/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6103 - accuracy: 0.6598 - val_loss: 0.6178 - val_accuracy: 0.6435\n",
            "Epoch 420/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6100 - accuracy: 0.6598 - val_loss: 0.6296 - val_accuracy: 0.6320\n",
            "Epoch 421/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6102 - accuracy: 0.6596 - val_loss: 0.6156 - val_accuracy: 0.6512\n",
            "Epoch 422/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6102 - accuracy: 0.6596 - val_loss: 0.6172 - val_accuracy: 0.6429\n",
            "Epoch 423/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6104 - accuracy: 0.6594 - val_loss: 0.6103 - val_accuracy: 0.6545\n",
            "Epoch 424/500\n",
            "377/377 [==============================] - 3s 7ms/step - loss: 0.6097 - accuracy: 0.6600 - val_loss: 0.6089 - val_accuracy: 0.6556\n",
            "Epoch 425/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6104 - accuracy: 0.6598 - val_loss: 0.6192 - val_accuracy: 0.6490\n",
            "Epoch 426/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6102 - accuracy: 0.6599 - val_loss: 0.6226 - val_accuracy: 0.6400\n",
            "Epoch 427/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6100 - accuracy: 0.6595 - val_loss: 0.6334 - val_accuracy: 0.6258\n",
            "Epoch 428/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6098 - accuracy: 0.6603 - val_loss: 0.6197 - val_accuracy: 0.6472\n",
            "Epoch 429/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6100 - accuracy: 0.6591 - val_loss: 0.6108 - val_accuracy: 0.6522\n",
            "Epoch 430/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6099 - accuracy: 0.6600 - val_loss: 0.6047 - val_accuracy: 0.6648\n",
            "Epoch 431/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6101 - accuracy: 0.6592 - val_loss: 0.6237 - val_accuracy: 0.6387\n",
            "Epoch 432/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6099 - accuracy: 0.6602 - val_loss: 0.6224 - val_accuracy: 0.6392\n",
            "Epoch 433/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6099 - accuracy: 0.6599 - val_loss: 0.6171 - val_accuracy: 0.6436\n",
            "Epoch 434/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6101 - accuracy: 0.6600 - val_loss: 0.6117 - val_accuracy: 0.6552\n",
            "Epoch 435/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6100 - accuracy: 0.6601 - val_loss: 0.6128 - val_accuracy: 0.6514\n",
            "Epoch 436/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6097 - accuracy: 0.6600 - val_loss: 0.6223 - val_accuracy: 0.6410\n",
            "Epoch 437/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6101 - accuracy: 0.6601 - val_loss: 0.6237 - val_accuracy: 0.6372\n",
            "Epoch 438/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6101 - accuracy: 0.6598 - val_loss: 0.6166 - val_accuracy: 0.6457\n",
            "Epoch 439/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6101 - accuracy: 0.6594 - val_loss: 0.6227 - val_accuracy: 0.6420\n",
            "Epoch 440/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6101 - accuracy: 0.6607 - val_loss: 0.6009 - val_accuracy: 0.6659\n",
            "Epoch 441/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6097 - accuracy: 0.6608 - val_loss: 0.6079 - val_accuracy: 0.6584\n",
            "Epoch 442/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6097 - accuracy: 0.6601 - val_loss: 0.6311 - val_accuracy: 0.6303\n",
            "Epoch 443/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6096 - accuracy: 0.6607 - val_loss: 0.6006 - val_accuracy: 0.6690\n",
            "Epoch 444/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6103 - accuracy: 0.6591 - val_loss: 0.6197 - val_accuracy: 0.6410\n",
            "Epoch 445/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6100 - accuracy: 0.6595 - val_loss: 0.6160 - val_accuracy: 0.6475\n",
            "Epoch 446/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6095 - accuracy: 0.6597 - val_loss: 0.6160 - val_accuracy: 0.6464\n",
            "Epoch 447/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6098 - accuracy: 0.6600 - val_loss: 0.6082 - val_accuracy: 0.6564\n",
            "Epoch 448/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6098 - accuracy: 0.6600 - val_loss: 0.6201 - val_accuracy: 0.6454\n",
            "Epoch 449/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6099 - accuracy: 0.6599 - val_loss: 0.6131 - val_accuracy: 0.6521\n",
            "Epoch 450/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6104 - accuracy: 0.6595 - val_loss: 0.6127 - val_accuracy: 0.6535\n",
            "Epoch 451/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6098 - accuracy: 0.6603 - val_loss: 0.6104 - val_accuracy: 0.6549\n",
            "Epoch 452/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6099 - accuracy: 0.6595 - val_loss: 0.6342 - val_accuracy: 0.6277\n",
            "Epoch 453/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6099 - accuracy: 0.6599 - val_loss: 0.6087 - val_accuracy: 0.6582\n",
            "Epoch 454/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6100 - accuracy: 0.6603 - val_loss: 0.6109 - val_accuracy: 0.6552\n",
            "Epoch 455/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6100 - accuracy: 0.6598 - val_loss: 0.6080 - val_accuracy: 0.6557\n",
            "Epoch 456/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6092 - accuracy: 0.6600 - val_loss: 0.6111 - val_accuracy: 0.6556\n",
            "Epoch 457/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6101 - accuracy: 0.6598 - val_loss: 0.6111 - val_accuracy: 0.6546\n",
            "Epoch 458/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6101 - accuracy: 0.6597 - val_loss: 0.6097 - val_accuracy: 0.6548\n",
            "Epoch 459/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6097 - accuracy: 0.6608 - val_loss: 0.6214 - val_accuracy: 0.6404\n",
            "Epoch 460/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6099 - accuracy: 0.6603 - val_loss: 0.6264 - val_accuracy: 0.6346\n",
            "Epoch 461/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6096 - accuracy: 0.6602 - val_loss: 0.6177 - val_accuracy: 0.6432\n",
            "Epoch 462/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6095 - accuracy: 0.6600 - val_loss: 0.6082 - val_accuracy: 0.6573\n",
            "Epoch 463/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6099 - accuracy: 0.6603 - val_loss: 0.6237 - val_accuracy: 0.6388\n",
            "Epoch 464/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6097 - accuracy: 0.6594 - val_loss: 0.6258 - val_accuracy: 0.6357\n",
            "Epoch 465/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6099 - accuracy: 0.6599 - val_loss: 0.6212 - val_accuracy: 0.6420\n",
            "Epoch 466/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6095 - accuracy: 0.6604 - val_loss: 0.6106 - val_accuracy: 0.6546\n",
            "Epoch 467/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6097 - accuracy: 0.6601 - val_loss: 0.6256 - val_accuracy: 0.6392\n",
            "Epoch 468/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6096 - accuracy: 0.6604 - val_loss: 0.6326 - val_accuracy: 0.6277\n",
            "Epoch 469/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6096 - accuracy: 0.6604 - val_loss: 0.6196 - val_accuracy: 0.6412\n",
            "Epoch 470/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6094 - accuracy: 0.6607 - val_loss: 0.6192 - val_accuracy: 0.6425\n",
            "Epoch 471/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6098 - accuracy: 0.6599 - val_loss: 0.6196 - val_accuracy: 0.6423\n",
            "Epoch 472/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6099 - accuracy: 0.6599 - val_loss: 0.6236 - val_accuracy: 0.6366\n",
            "Epoch 473/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6100 - accuracy: 0.6597 - val_loss: 0.6079 - val_accuracy: 0.6562\n",
            "Epoch 474/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6096 - accuracy: 0.6599 - val_loss: 0.6225 - val_accuracy: 0.6405\n",
            "Epoch 475/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6096 - accuracy: 0.6608 - val_loss: 0.6247 - val_accuracy: 0.6383\n",
            "Epoch 476/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6095 - accuracy: 0.6600 - val_loss: 0.6246 - val_accuracy: 0.6392\n",
            "Epoch 477/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6098 - accuracy: 0.6600 - val_loss: 0.6039 - val_accuracy: 0.6625\n",
            "Epoch 478/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6100 - accuracy: 0.6603 - val_loss: 0.6010 - val_accuracy: 0.6676\n",
            "Epoch 479/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6095 - accuracy: 0.6603 - val_loss: 0.6261 - val_accuracy: 0.6354\n",
            "Epoch 480/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6095 - accuracy: 0.6603 - val_loss: 0.6180 - val_accuracy: 0.6499\n",
            "Epoch 481/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6093 - accuracy: 0.6611 - val_loss: 0.6230 - val_accuracy: 0.6418\n",
            "Epoch 482/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6096 - accuracy: 0.6604 - val_loss: 0.6143 - val_accuracy: 0.6493\n",
            "Epoch 483/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6100 - accuracy: 0.6596 - val_loss: 0.6287 - val_accuracy: 0.6391\n",
            "Epoch 484/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6097 - accuracy: 0.6600 - val_loss: 0.6163 - val_accuracy: 0.6507\n",
            "Epoch 485/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6093 - accuracy: 0.6599 - val_loss: 0.6196 - val_accuracy: 0.6452\n",
            "Epoch 486/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6095 - accuracy: 0.6603 - val_loss: 0.6220 - val_accuracy: 0.6389\n",
            "Epoch 487/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6096 - accuracy: 0.6604 - val_loss: 0.6134 - val_accuracy: 0.6495\n",
            "Epoch 488/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6095 - accuracy: 0.6607 - val_loss: 0.6094 - val_accuracy: 0.6594\n",
            "Epoch 489/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6094 - accuracy: 0.6602 - val_loss: 0.6063 - val_accuracy: 0.6590\n",
            "Epoch 490/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6094 - accuracy: 0.6598 - val_loss: 0.6347 - val_accuracy: 0.6266\n",
            "Epoch 491/500\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.6095 - accuracy: 0.6608 - val_loss: 0.6071 - val_accuracy: 0.6582\n",
            "Epoch 492/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6093 - accuracy: 0.6606 - val_loss: 0.6081 - val_accuracy: 0.6619\n",
            "Epoch 493/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6098 - accuracy: 0.6596 - val_loss: 0.6199 - val_accuracy: 0.6467\n",
            "Epoch 494/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6094 - accuracy: 0.6601 - val_loss: 0.6075 - val_accuracy: 0.6592\n",
            "Epoch 495/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6095 - accuracy: 0.6604 - val_loss: 0.6252 - val_accuracy: 0.6336\n",
            "Epoch 496/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6092 - accuracy: 0.6609 - val_loss: 0.6094 - val_accuracy: 0.6543\n",
            "Epoch 497/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6095 - accuracy: 0.6600 - val_loss: 0.6328 - val_accuracy: 0.6290\n",
            "Epoch 498/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6095 - accuracy: 0.6605 - val_loss: 0.6078 - val_accuracy: 0.6648\n",
            "Epoch 499/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6092 - accuracy: 0.6612 - val_loss: 0.6137 - val_accuracy: 0.6489\n",
            "Epoch 500/500\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 0.6097 - accuracy: 0.6596 - val_loss: 0.6087 - val_accuracy: 0.6577\n",
            "Test loss: 0.609903872013092\n",
            "Test accuracy: 0.6561038494110107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLyn47s0vrjw",
        "colab_type": "code",
        "outputId": "60e3168a-b25f-438d-e564-b2f1b8ddead1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Zapisanie modelu sieci\n",
        "pruned_model.save('Model_Sieci_Glebokiego_Uczenia')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: Model_Sieci_Glebokiego_Uczenia/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0QlDlE_sWsi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "394328f1-b9ff-442d-9651-599f8b6b4602"
      },
      "source": [
        "# Wczytanie modelu sieci\n",
        "siec = tf.keras.models.load_model('Model_Sieci_Glebokiego_Uczenia')\n",
        "\n",
        "import seaborn as sns\n",
        "y_pred=siec.predict_classes(train_data['x'])\n",
        "con_mat = tf.math.confusion_matrix(labels=train_data['y'], predictions=y_pred).numpy()\n",
        "\n",
        "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        " \n",
        "con_mat_df = pd.DataFrame(con_mat_norm)\n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}